{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector <a id='top'></a>\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "_Using Amazon's SageMaker for Train | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "This project is about a classification model that examines a text file from news and performs binary classification; labeling that news as either fake or real. The model was trained using a dataset from [kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset). The dataset consists of about 21000 news labeled as true and about 23000 news categorized as fake.\n",
    "\n",
    "The project is inspired from a [research](https://blogs.scientificamerican.com/beautiful-minds/liberals-and-conservatives-are-both-susceptible-to-fake-news-but-for-different-reasons/?WT.mc_id=send-to-friend) that suggests both liberals and conservatives are motivated to believe fake news, and dismiss real news that contradicts their ideologies. Fake news spread through social media has become a serious problem and this study aims to build an unbiased model that could detect news as fake or real.\n",
    "\n",
    "The first step in working with any dataset is loading the data in and noting what information is included in the dataset. This is an important step in eventually working with the data, and knowing what kinds of features we have to work with as we transform and group the data. So, this notebook is all about exploring the data and noting patterns about the features we are given and the distribution of data.\n",
    "\n",
    "## General Outline\n",
    "\n",
    "1. [Import Libraries](#import)\n",
    "2. [Read in the Data](#read)\n",
    "3. [Prepare and Process the Data](#prepare)\n",
    "4. [Check Data](#check)\n",
    "5. [Explore the Data](#explore)\n",
    "    1. [Distribution of the Classes](#classes)\n",
    "    2. [Distribution of the News Topics](#topics)\n",
    "    3. [Distribution of the News Topics and Classes](#topics-classes)\n",
    "    4. [Explore Date](#date)\n",
    "    5. [Explore Text](#text)\n",
    "    6. [Explore Words](#words)\n",
    "        1. [Word Clouds in Fake and True News](#wordcloud)\n",
    "        2. [Most Frequent Words in Fake and True News](#word-frequency)\n",
    "6. [Next Notebook](#next)\n",
    "\n",
    "## Import Libraries <a id='import'></a>\n",
    "\n",
    "In the next cells we will import the required libraries for this analysis and set some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chart_studio'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/83/8gchw24s00v8m6yk0fmzctnr0000gn/T/ipykernel_69759/1755066839.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mPIL\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mchart_studio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplotly\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mplotly\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgraph_objects\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mgo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mplotly\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpress\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'chart_studio'"
     ]
    }
   ],
   "source": [
    "# Install libraries.\n",
    "# !pip install chart_studio\n",
    "# !pip install wordcloud\n",
    "\n",
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Set Plotly theme.\n",
    "pio.templates.default = \"gridon\"\n",
    "\n",
    "# Set global variables.\n",
    "RANDOM_STATE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#top)\n",
    "\n",
    "## Read in the Data <a id='read'></a>\n",
    "\n",
    "The cell below will load the data into `pandas` dataframes.\n",
    "\n",
    "> **Acknowledgements for data**:\n",
    ">- Ahmed H, Traore I, Saad S. “Detecting opinion spams and fake news using text classification”, Journal of Security and Privacy, Volume 1, Issue 1, Wiley, January/February 2018.\n",
    ">- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n",
    "\n",
    "The dataset is made of multiple text strings and other characteristics summarized in `csv` files named `Fake.csv` and `True.csv`, which we can read in using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/True.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/83/8gchw24s00v8m6yk0fmzctnr0000gn/T/ipykernel_69759/1332053067.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"data/True.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mfake\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"data/Fake.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Show first rows for each dataset.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    678\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 680\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    681\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 575\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    576\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    577\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    931\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    932\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 933\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    934\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1215\u001B[0m             \u001B[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1216\u001B[0m             \u001B[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1217\u001B[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[1;32m   1218\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1219\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    787\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 789\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    790\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    791\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/True.csv'"
     ]
    }
   ],
   "source": [
    "true = pd.read_csv(\"data/True.csv\")\n",
    "fake = pd.read_csv(\"data/Fake.csv\")\n",
    "\n",
    "# Show first rows for each dataset.\n",
    "display(true.head())\n",
    "display(fake.head())\n",
    "\n",
    "# Print the number of real and fake news.\n",
    "print('\\nThere are {} real and {} fake news'.format(true.shape[0], fake.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#top)\n",
    "\n",
    "## Prepare and Process the Data <a id='prepare'></a>\n",
    "\n",
    "It is more convenient to merge the two datasets and then apply any processing tasks and text transformations to the new dataframe. First, we need to create a new column `label` to save the labels of each text. We will also shuffle the final dataframe and then delete the previous datasets to free up some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'label' column.\n",
    "true['label'] = 'True'\n",
    "fake['label'] = 'Fake'\n",
    "\n",
    "# Concatenate the 2 dfs.\n",
    "df = pd.concat([true, fake])\n",
    "\n",
    "# To save a bit of memory we can set fake and true to None.\n",
    "fake = true = None\n",
    "\n",
    "#  Shuffle data.\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Show first rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#top)\n",
    "\n",
    "## Check Data <a id='check'></a>\n",
    "\n",
    "The dataframe will be examined for the quality of its data. The types and shape of the data will be checked, as well as if there are any missing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- There are no missing values.\n",
    "- Dates were recognized as strings. Later, we will use a `pandas` date parser function to recognize dates correctly.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "## Explore the Data <a id='explore'></a>\n",
    "\n",
    "Next, let's look at the distribution of data.\n",
    "\n",
    "### Distribution of the Classes <a id='classes'></a>\n",
    "\n",
    "We need to check how evenly is our data distributed among the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show counts for each class.\n",
    "fig = px.bar(df.groupby('label').count().reset_index(), x='label', y='title', text='title', opacity=0.6)\n",
    "fig.update_layout(title_text='Distribution of News')\n",
    "fig.update_xaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- It seems that the dataset is balanced, so no actions needed to handle any imbalances between the classes.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "### Distribution of the News Topics <a id='topics'></a>\n",
    "\n",
    "It may also be helpful to look at the `subject` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show counts for each class.\n",
    "fig = px.bar(df.groupby('subject').count()['title'].reset_index().sort_values(by='title'),\n",
    "             x='subject', y='title', text='title', opacity=0.6)\n",
    "fig.update_layout(title_text='Distribution of News Subjects')\n",
    "fig.update_xaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- We have 5 categories with a lot of `True` and `Fake` news and 3 with only a few hundres.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "### Distribution of the News Topics and Classes <a id='topics-classes'></a>\n",
    "\n",
    "Let's dig deeper and see the distribution of labels inside each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.groupby(['label', 'subject']).count().reset_index()\n",
    "fig = px.bar(df_sum, x='label', y='title', color='subject', text='title', opacity=0.6)\n",
    "fig.update_xaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- It seems that the `Fake` and `Real` news datasets do not contain the same topics in the `subject` category. Possibly, `politics` is similar to `politicsNews`, but since `subjects` are mapped differently between `True` and `Fake` news, it would be better to remove this feature from our model.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "### Explore Date <a id='date'></a>\n",
    "\n",
    "It would be interesting to see if there are any patterns in `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date str into date object. Take care of any errors for invalid dates.\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df_date = df.groupby(['label', 'date'])['title'].count().reset_index()\n",
    "\n",
    "fig = px.line(df_date, x='date', y='title', color='label')\n",
    "fig.update_xaxes(title_text=None)\n",
    "fig.update_yaxes(title_text=None)\n",
    "fig.update_layout(legend_title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- Not too much to infer from this time series plot.\n",
    "- Let's extract the week of the year, month of the year and other date features to check if there is any seasonality. It would be wise to compare the date features at the same window. It seems that the more `True` news observed after August 2017 is not because of the date. For the same reason, the lack of `True` news before January 2016 is not due to the date. In other words, we cannot infer that all the news before January 2016 were `Fake`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df based on date.\n",
    "df_filtered = df[(df['date'] < '2017-08-31') & (df['date'] > '2016-02-01')].copy()\n",
    "df_filtered.loc[:, 'weekday'] = df_filtered['date'].dt.dayofweek\n",
    "df_filtered.loc[:, 'week'] = df_filtered['date'].dt.weekofyear\n",
    "df_filtered.loc[:, 'month'] = df_filtered['date'].dt.month\n",
    "df_filtered.loc[:, 'quarter'] = df_filtered['date'].dt.quarter\n",
    "\n",
    "df_weekday = df_filtered.groupby(['label', 'weekday']).count()['title'].reset_index()\n",
    "\n",
    "fig = px.line(df_weekday, x='weekday', y='title', color='label')\n",
    "fig.update_layout(title_text='Day of Week')\n",
    "fig.update_xaxes(title_text=None)\n",
    "fig.update_yaxes(title_text=None)\n",
    "fig.update_layout(legend_title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week = df_filtered.groupby(['label', 'week']).count()['title'].reset_index()\n",
    "\n",
    "fig = px.line(df_week, x='week', y='title', color='label')\n",
    "fig.update_layout(title_text='Week of the Year')\n",
    "fig.update_xaxes(title_text=None)\n",
    "fig.update_yaxes(title_text=None)\n",
    "fig.update_layout(legend_title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = df_filtered.groupby(['label', 'month']).count()['title'].reset_index()\n",
    "\n",
    "fig = px.line(df_month, x='month', y='title', color='label')\n",
    "fig.update_layout(title_text='Monthly')\n",
    "fig.update_xaxes(title_text=None)\n",
    "fig.update_yaxes(title_text=None)\n",
    "fig.update_layout(legend_title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quarter = df_filtered.groupby(['label', 'quarter']).count()['title'].reset_index()\n",
    "\n",
    "fig = px.line(df_quarter, x='quarter', y='title', color='label')\n",
    "fig.update_layout(title_text='Quarterly')\n",
    "fig.update_xaxes(title_text=None)\n",
    "fig.update_yaxes(title_text=None)\n",
    "fig.update_layout(legend_title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- There is no clear distinction between date features in `Fake` and `True` news.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "### Explore Text <a id='text'></a>\n",
    "\n",
    "Let's print a couple of `True` and `Fake` examples to check if there are any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fake News\\n')\n",
    "print(df[df.label == 'Fake']['text'].tolist()[3])\n",
    "print()\n",
    "print(df[df.label == 'Fake']['text'].tolist()[5])\n",
    "print()\n",
    "print('\\n\\nTrue News\\n')\n",
    "print(df[df.label == 'True']['text'].tolist()[0])\n",
    "print()\n",
    "print(df[df.label == 'True']['text'].tolist()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- Maybe `True` news contain more dates, numbers, and names than `Fake` ones. Later, we will extract these features and use them as input in the models.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "### Explore Words <a id='words'></a>\n",
    "\n",
    "It is interesting to see what kind of words and phrases are commonly used in `Fake` and `True` news. Before this exploration, a good strategy is to clean the text applying the following steps:\n",
    "\n",
    "- Read a text file as a string of raw text.\n",
    "- Lower case all words, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "- Normalize numbers, replacing them with the text `number`.\n",
    "- Remove non-words, remove punctuation, and trim all white spaces (tabs, newlines, spaces) to a single space character.\n",
    "- Tokenize the raw text string into a list of words where each entry is a word. Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. The tokens become the input for another process like parsing and text mining. Then the words will be ready to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "- Use lemmatization or stemming to consolidate closely redundant words. For example, \"discount\", \"discounts\", \"discounted\" and \"discounting\" will be all replaced with \"discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so \"include\", \"includes\", \"included\", and \"including\" are all replaced with \"includ\".\n",
    "- Remove stopwords. Stop words are so frequently used that for many tasks (but not all) they don't carry much information. Examples are \"any\", \"all\", \"what\", etc. NLTK has an inbuilt corpus of english stopwords that can be loaded and used.\n",
    "- Apply additional text preparation steps, such as normalizing links and emails: All https and http links will be replaced with the text \"link\" and all emails will be replaced with the text \"email\".\n",
    "- Render either a word cloud or a bar chart with the most frequent unigrams, bigrams, trigrams, etc.\n",
    "\n",
    "#### Word Clouds in Fake and True News <a id='wordcloud'></a>\n",
    "\n",
    "First, we can use directly the `WordCloud` module before doing any heavy text processing, so as to get an idea of the most important words or phrases in fake and true news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create a word cloud.\n",
    "def make_wordcloud(text, mask, color):\n",
    "    wordcloud = WordCloud(max_words=200, mask=mask,\n",
    "                          background_color='white',\n",
    "                          contour_width=2,\n",
    "                          contour_color=color).generate(text)\n",
    "    plt.figure(figsize=(17,12))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Read an image in order to use it as a shape for our word cloud.\n",
    "fake_mask = np.array(Image.open(\"data/fake.png\"))\n",
    "true_mask = np.array(Image.open(\"data/true.png\"))\n",
    "\n",
    "# Get the fake and true news.\n",
    "fake_text = \" \".join(text for text in df[df.label == 'Fake']['text'])\n",
    "true_text = \" \".join(text for text in df[df.label == 'True']['text'])\n",
    "\n",
    "# Render word clouds.\n",
    "make_wordcloud(fake_text, fake_mask, 'blue')\n",
    "make_wordcloud(true_text, true_mask, 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- Fake news contain a lot of words like Donald Trump, Hillary Clinton, White House, and United States.\n",
    "- True news contains a lot of the words found in fake, but also contains a lot of dates like on Tuesday, on Monday, on Sunday, last week, etc.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "#### Most Frequent Words in Fake and True News <a id='word-frequency'></a>\n",
    "\n",
    "Let's apply the text processing steps described before and then print the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'tqdm' instance to time and estimate the progress of functions.\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create a function to clean and prepare text.\n",
    "def clean_text(text):\n",
    "    \"\"\" Remove any punctuation, numbers, newlines, and stopwords.\n",
    "    Convert to lower case.\n",
    "    Split the text string into individual words, stem each word,\n",
    "    and append the stemmed word to words. Make sure there's a single\n",
    "    space between each stemmed word.\n",
    "    Args:\n",
    "        text: text, string\n",
    "    Returns:\n",
    "        words: cleaned words, list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace numbers with the str 'number'.\n",
    "    text = re.sub('\\d+', 'number', text)\n",
    "    \n",
    "    # Replace newlines with spaces.\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    \n",
    "    # Replace punctuation with spaces.\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Replace links with the str 'link'\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                   'link', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Replace emails with the str 'email'\n",
    "    text = re.sub('\\S+@\\S+', 'email', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Convert all letters to lower case.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Create the stemmer.\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Split text into words.\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords.\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    # Stem words.\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Apply the cleaning function to the dataset.\n",
    "df.text = df.text.progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to count and return the most frequent words and then plot them in a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to count and return the most frequent words.\n",
    "def frequent_words(label, max_words):\n",
    "    # Gather text and concatenate.\n",
    "    text = df[df['label'] == label]['text'].values\n",
    "    text = np.concatenate(text)\n",
    "    \n",
    "    # Count words.\n",
    "    counts = Counter(text)\n",
    "    \n",
    "    # Create a pandas df from the Counter dictionary.\n",
    "    df_counts = pd.DataFrame.from_dict(counts, orient='index')\n",
    "    df_counts = df_counts.rename(columns={0:'counts'})\n",
    "    \n",
    "    # Return a df with the most frequent words.\n",
    "    return df_counts.sort_values(by='counts', ascending=False).head(max_words).sort_values(by='counts')\n",
    "\n",
    "# Get the 50 most frequent words.\n",
    "df_fake_counts = frequent_words(label='Fake', max_words=50)\n",
    "df_true_counts = frequent_words(label='True', max_words=50)\n",
    "\n",
    "# Plot horizontal bar charts.\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Fake News\", \"True News\"))\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_fake_counts.counts.tolist(),\n",
    "                     y=df_fake_counts.index.values.tolist(),\n",
    "                     orientation='h', opacity=0.6), 1, 1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_true_counts.counts.tolist(),\n",
    "                     y=df_true_counts.index.values.tolist(),\n",
    "                     orientation='h', opacity=0.6), 1, 2)\n",
    "\n",
    "fig.update_layout(height=900, width=900, title_text=\"Most Frequent Words\", showlegend=False)\n",
    "fig.update_xaxes(showgrid=False, title_text=None)\n",
    "fig.update_yaxes(showgrid=False, title_text=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "- We got an idea of the most frequent unigrams and bigrams. We could further continue the analysis of trigrams or even higher n-grams, but the purpose of this study is to build a classification model.\n",
    "\n",
    "[Go to the top](#top)\n",
    "\n",
    "## Next Notebook <a id='next'></a>\n",
    "\n",
    "In the next [notebook](https://github.com/gtraskas/fake-news-detector/blob/master/fake-news-detector.ipynb), we will use these datasets to train a complete fake news classifier. We will extract meaningful features from the text, which we will use to train and deploy a classification model in an AWS SageMaker notebook instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}